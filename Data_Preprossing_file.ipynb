{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_sentence(sentence):\n",
        "    sentences = sent_tokenize(sentence)  # Tokenize into sentences\n",
        "    lemmatized_sentences = []\n",
        "\n",
        "    for sent in sentences:\n",
        "        words = word_tokenize(sent.lower())\n",
        "        lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "        lemmatized_sentence = ' '.join(lemmatized_words)\n",
        "        lemmatized_sentences.append(lemmatized_sentence)\n",
        "\n",
        "    return ' '.join(lemmatized_sentences)\n",
        "\n",
        "\n",
        "df['lemmatized_comments'] = df['Comment'].apply(lemmatize_sentence)\n",
        "\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sRUKKiBVTpda"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X=df['Comment']\n",
        "y=df['Majority_Sentiment']\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2pE-4-IfDGAq"
      },
      "outputs": [],
      "source": [
        "csv_file_path = '/content/lemmatized_sentences.csv'  # Replace 'your_file_path.csv' with your desired file path\n",
        "\n",
        "# Save the DataFrame to the CSV file\n",
        "df.to_csv(csv_file_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2uPxscj_UTul",
        "outputId": "fa73267a-66e6-4133-df42-8a0a5138b878"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "64787              Lol. This brings back so many memories. \n",
            "128761    This is something both Islamist, secularists a...\n",
            "56970     damn, I wasn't expecting you to literally admi...\n",
            "55118     Their obsession with us beyond everything  I h...\n",
            "100987    I will watch it then. \\n\\nAbout films, our son...\n",
            "                                ...                        \n",
            "128106     *dal chara bhat hobe but bhat chara dal hobe na*\n",
            "103694    1. what part of hate we see on FB is due to it...\n",
            "860           this is for americans wanting to help afghans\n",
            "15795     Interesting, thanks.  I looked up a reference ...\n",
            "121958    Not only is that statement inaccurate but it i...\n",
            "Name: Comment, Length: 104492, dtype: object\n"
          ]
        }
      ],
      "source": [
        "print(X_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmTk2OiQ98Ke",
        "outputId": "189a7300-f927-4ef8-8fd0-9f01cebeb303"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "!pip install nltk\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKJhh_DLsdry",
        "outputId": "9aab80d2-00c6-4458-d729-e5915869bb26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        Unnamed: 0  Post_ID Comment_ID  \\\n",
            "0                1  108l3ho    j3vlnat   \n",
            "1                2  10j6oqj    j5j34zb   \n",
            "2                3  10j6oqj    j5jbs3f   \n",
            "3                4  10j6oqj    j5jfczd   \n",
            "4                5  10o9tvi    j6h1ko8   \n",
            "...            ...      ...        ...   \n",
            "130611      166838   zp46e8          0   \n",
            "130612      166839   zp46e8          0   \n",
            "130613      166840   zp46e8          0   \n",
            "130614      166841   zp46e8          0   \n",
            "130615      166842   zp46e8          0   \n",
            "\n",
            "                                                  Comment Majority_Sentiment  \n",
            "0       Some of them yes but this one i got it from my...            Neutral  \n",
            "1             I keep mine in my phone cover for good luck           Positive  \n",
            "2       Hang on to it! These are rare to come by as th...            Neutral  \n",
            "3       Yeah?! Cool, I got this from my classmate in 2...           Positive  \n",
            "4       Check out the Kaja Throm! Beautiful marketplac...           Positive  \n",
            "...                                                   ...                ...  \n",
            "130611  Our population growth rate is 1.9. Our Birth r...            Neutral  \n",
            "130612  I obviously won't. But - not only you don't do...           Negative  \n",
            "130613  bengal has always been some of the most fertil...           Positive  \n",
            "130614  >racist agenda\\n\\nAhhh yes, it's always racism...           Negative  \n",
            "130615  >Ahhh yes, it's always racism.\\n\\nWhat else ho...           Negative  \n",
            "\n",
            "[130616 rows x 5 columns]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df=pd.read_csv('/content/raw_reddit_data_filtered.csv')\n",
        "print(df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "go_2rpjbnqNc"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import string\n",
        "def remove_punctuation(text):\n",
        "    # Use a regular expression to remove punctuation\n",
        "      return re.sub(f'[{string.punctuation}]', '', text)\n",
        "\n",
        "# Apply the remove_punctuation function to the 'text' column\n",
        "new_df['Comment'] = new_df['Comment'].apply(remove_punctuation)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1GDB0eTapmAi",
        "outputId": "9d265869-2b90-4faa-a2b6-5158c2267f07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        Unnamed: 0  Post_ID Comment_ID  \\\n",
            "0                1  108l3ho    j3vlnat   \n",
            "1                2  10j6oqj    j5j34zb   \n",
            "2                3  10j6oqj    j5jbs3f   \n",
            "3                4  10j6oqj    j5jfczd   \n",
            "4                5  10o9tvi    j6h1ko8   \n",
            "...            ...      ...        ...   \n",
            "130611      166838   zp46e8          0   \n",
            "130612      166839   zp46e8          0   \n",
            "130613      166840   zp46e8          0   \n",
            "130614      166841   zp46e8          0   \n",
            "130615      166842   zp46e8          0   \n",
            "\n",
            "                                                  Comment Majority_Sentiment  \\\n",
            "0       Some of them yes but this one i got it from my...            Neutral   \n",
            "1             I keep mine in my phone cover for good luck           Positive   \n",
            "2       Hang on to it These are rare to come by as the...            Neutral   \n",
            "3       Yeah Cool I got this from my classmate in 2015...           Positive   \n",
            "4       Check out the Kaja Throm Beautiful marketplace...           Positive   \n",
            "...                                                   ...                ...   \n",
            "130611  Our population growth rate is 19 Our Birth rat...            Neutral   \n",
            "130612  I obviously wont But  not only you dont do bes...           Negative   \n",
            "130613  bengal has always been some of the most fertil...           Positive   \n",
            "130614  racist agenda\\n\\nAhhh yes its always racism\\n\\...           Negative   \n",
            "130615  Ahhh yes its always racism\\n\\nWhat else honest...           Negative   \n",
            "\n",
            "                                     lemmatized_sentences  \\\n",
            "0       some of them yes but this one i got it from my...   \n",
            "1             i keep mine in my phone cover for good luck   \n",
            "2       hang on to it ! these are rare to come by a th...   \n",
            "3       yeah ? ! cool , i got this from my classmate i...   \n",
            "4       check out the kaja throm ! beautiful marketpla...   \n",
            "...                                                   ...   \n",
            "130611  our population growth rate is 1.9. our birth r...   \n",
            "130612  i obviously wo n't . but - not only you do n't...   \n",
            "130613  bengal ha always been some of the most fertile...   \n",
            "130614  > racist agenda ahhh yes , it 's always racism...   \n",
            "130615  > ahhh yes , it 's always racism . what else h...   \n",
            "\n",
            "                                      lemmatized_comments  \n",
            "0       some of them yes but this one i got it from my...  \n",
            "1             i keep mine in my phone cover for good luck  \n",
            "2       hang on to it  these are rare to come by a the...  \n",
            "3       yeah   cool  i got this from my classmate in 2...  \n",
            "4       check out the kaja throm  beautiful marketplac...  \n",
            "...                                                   ...  \n",
            "130611  our population growth rate is 19  our birth ra...  \n",
            "130612  i obviously wo nt  but  not only you do nt do ...  \n",
            "130613  bengal ha always been some of the most fertile...  \n",
            "130614   racist agenda ahhh yes  it s always racism  t...  \n",
            "130615   ahhh yes  it s always racism  what else hones...  \n",
            "\n",
            "[130616 rows x 7 columns]\n"
          ]
        }
      ],
      "source": [
        "print(new_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AsVlLk11PfzG"
      },
      "outputs": [],
      "source": [
        "new_df.to_csv('preprocessed_data.csv', encoding='utf-8')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gkSF6lCEqZ5J"
      },
      "outputs": [],
      "source": [
        "new_df = new_df.drop('lemmatized_sentences', axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-brAmdY-l-uT"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "new_df=pd.read_csv('/content/lemmatized_sentences (1).csv')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
